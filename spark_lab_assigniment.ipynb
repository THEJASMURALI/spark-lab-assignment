{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spark lab assigniment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxyKGbHU-UyN",
        "outputId": "9931f0a2-6903-45f4-eef1-d2f934d98c14"
      },
      "source": [
        "!apt-get update\r\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\r\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\r\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Connecting to cloud.r-pr\r0% [1 InRelease gpgv 88.7 kB] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Connected to cloud.r-proje\r                                                                               \rHit:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rIgn:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r                                                                               \r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Waiting for headers]\r                                                                         \rHit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Waiting for headers]\r                                                                         \rHit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Connecting to ppa.launchpa\r                                                                               \rHit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Connecting to ppa.launchpad.net (91.189.95.85)] \r                                                                               \rHit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDKy9VcK-lGO"
      },
      "source": [
        "!tar xf spark-2.3.1-bin-hadoop2.7.tgz\r\n",
        "!pip install -q findspark\r\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHE5A_UE-tfy",
        "outputId": "3bd13567-2e4c-453b-c2d3-11eeca507282"
      },
      "source": [
        "import os\r\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\r\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.1-bin-hadoop2.7\"\r\n",
        "\r\n",
        "!ls\r\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number.txt\t\t   spark-2.3.1-bin-hadoop2.7.tgz    textfile.txt\n",
            "sample_data\t\t   spark-2.3.1-bin-hadoop2.7.tgz.1\n",
            "spark-2.3.1-bin-hadoop2.7  spark-warehouse\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSQRqVSq-xCl"
      },
      "source": [
        "import findspark\r\n",
        "findspark.init()\r\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "aSuB7KaC-z5v",
        "outputId": "c2be51c8-4d98-4907-ddca-a6161ff14729"
      },
      "source": [
        "import pyspark\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "spark = SparkSession.builder.appName(\"application\").getOrCreate() \r\n",
        "spark"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://7a37e32880f0:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>application</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f936d9b1240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GicKNQob-6Bg",
        "outputId": "a5792fcd-66e0-4a0e-a906-d48296a27635"
      },
      "source": [
        "df = spark.createDataFrame([{\"spark on\": \"colab\"} for x in range(1000)])\r\n",
        "df.show(1)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/session.py:340: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
            "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+--------+\n",
            "|spark on|\n",
            "+--------+\n",
            "|   colab|\n",
            "+--------+\n",
            "only showing top 1 row\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_ahY9gOBm4g"
      },
      "source": [
        "#1.\tCreate RDDs in three different ways."
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XZgTSPA-2S9",
        "outputId": "388d0d84-de7a-4c2d-8b19-6db8c5929ca7"
      },
      "source": [
        "#1st method\r\n",
        "text=\"hello. wellcome to manipal school of information science. there are so many branch are there, like ML, BDA, CLOUD, VLSI, HBA.\".split(\" \")\r\n",
        "words = spark.sparkContext.parallelize(text,2)\r\n",
        "type(words)\r\n",
        "words.collect()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello.',\n",
              " 'wellcome',\n",
              " 'to',\n",
              " 'manipal',\n",
              " 'school',\n",
              " 'of',\n",
              " 'information',\n",
              " 'science.',\n",
              " 'there',\n",
              " 'are',\n",
              " 'so',\n",
              " 'many',\n",
              " 'branch',\n",
              " 'are',\n",
              " 'there,',\n",
              " 'like',\n",
              " 'ML,',\n",
              " 'BDA,',\n",
              " 'CLOUD,',\n",
              " 'VLSI,',\n",
              " 'HBA.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qE4U-ci6_J0w",
        "outputId": "8f61fe68-e96d-4123-d802-4e43ed7308dd"
      },
      "source": [
        "#2nd method\r\n",
        "rdd_2=spark.sparkContext.textFile(\"textfile.txt\")\r\n",
        "rdd_2.collect()\r\n",
        "type(rdd_2)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.RDD"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDvs54Ah_Xru",
        "outputId": "a6c96d5f-1836-4e70-e126-6ccdccbcd23f"
      },
      "source": [
        "#3rd method\r\n",
        "rdd_3=words.filter(lambda word:word.startswith('s'))\r\n",
        "rdd_3.collect()\r\n",
        "type(rdd_3)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.PipelinedRDD"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLBqT403_fBf"
      },
      "source": [
        "#2.Read a text file and count the number of words in the file using RDD operations."
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pM5Et6j3_pmH",
        "outputId": "c4547fde-c83f-4be8-f79b-ddabc468f3a9"
      },
      "source": [
        "word_count=rdd_2.flatMap(lambda s:s.split(\" \"))\r\n",
        "word_count.count()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeBRP2HO_r4f"
      },
      "source": [
        "#3.Write a program to find the word frequency in a given file."
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZWYrMDy_vWX",
        "outputId": "c65a57a7-d255-4b42-85b7-7844e0fe7171"
      },
      "source": [
        "info=spark.sparkContext.textFile(\"textfile.txt\")\r\n",
        "info=info.flatMap(lambda x:x.split())\r\n",
        "content_map=info.map(lambda c:(c,1))\r\n",
        "content_map.reduceByKey(lambda a,b:a+b).collect()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('school', 1),\n",
              " ('of', 1),\n",
              " ('science.', 1),\n",
              " ('there', 1),\n",
              " ('are', 2),\n",
              " (\"branch's\", 1),\n",
              " ('there,', 1),\n",
              " ('like', 1),\n",
              " ('ML,', 1),\n",
              " ('BDA,', 1),\n",
              " ('CLOUD,', 1),\n",
              " ('HBA.', 1),\n",
              " ('hello.', 1),\n",
              " ('wellcome', 1),\n",
              " ('to', 1),\n",
              " ('manipal', 1),\n",
              " ('information', 1),\n",
              " ('so', 1),\n",
              " ('many', 1),\n",
              " ('VLSI,', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoNbm07z_yhT"
      },
      "source": [
        "#4.Write a program to convert all words in a file to uppercase."
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2Du3yoSABgV",
        "outputId": "b4a4678a-e6a6-4255-ad37-a89dc1b9d715"
      },
      "source": [
        "rdd_2.map(lambda c:c.upper()).collect()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['HELLO. WELLCOME TO MANIPAL SCHOOL OF INFORMATION SCIENCE.',\n",
              " \"THERE ARE SO MANY BRANCH'S ARE THERE, LIKE ML, BDA, CLOUD, VLSI, HBA.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKX2p9LLAcBD"
      },
      "source": [
        "5.Write a program to convert all words in a file to lowercase.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhAIVGv_AEYU"
      },
      "source": [
        "#5.Write a program to convert all words in a file to lowercase."
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFSoWm11AHZz",
        "outputId": "bde2b8ae-5e90-46dc-dd48-dda2728fbb09"
      },
      "source": [
        "rdd_2.map(lambda c:c.lower()).collect()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello. wellcome to manipal school of information science.',\n",
              " \"there are so many branch's are there, like ml, bda, cloud, vlsi, hba.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtmlkoIoARSa"
      },
      "source": [
        "6.Write a program to capitalize first letter of each words in file (use string capitalize() method).\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Mq-Kf47tAYJk",
        "outputId": "7591161a-d3c4-4c34-cbfb-6ee2ca5e3a57"
      },
      "source": [
        "capital=rdd_2.flatMap(lambda a:a.split(\" \")).map(lambda c:c.capitalize()).collect()\r\n",
        "\" \".join(capital)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Hello. Wellcome To Manipal School Of Information Science. There Are So Many Branch's Are There, Like Ml, Bda, Cloud, Vlsi, Hba.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rQRHLp1AqWL"
      },
      "source": [
        "7.Find the longest length of word from given set of words.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pxg81iQrAuJY",
        "outputId": "7cdb6858-eb16-4c1a-d9c9-407c61d5d8af"
      },
      "source": [
        "longest_word=rdd_2.flatMap(lambda x:x.split(\" \"))\r\n",
        "longest_word.map(lambda nu:(len(nu),nu)).max()[1]"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'information'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4tmcrz_AyyR"
      },
      "source": [
        "8.Map the Registration numbers to corresponding branch. 6000 series BDA, 9000 series HAD, 1000 series MS, 2000 series VLSI, 3000 series ES, 4000 series MSc, 5000 series CC. Given registration number, generate a key-value pair of Registration Number and Corresponding Branch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7kPRGxCBDvv",
        "outputId": "d848169d-3eb5-435d-d55c-04a30dd6199d"
      },
      "source": [
        "registration_number=[6027,2005,2035,6011,9007,9056,3088,3045,4088,4065,5077,5066,1001,1002]\r\n",
        "context=spark.sparkContext.parallelize(registration_number,2)\r\n",
        "classify=context.map(lambda reg:('VLSI',reg) if reg>2000 and reg<3000 \r\n",
        "        else ('MS',reg) if reg>1000 and reg<2000\r\n",
        "        else ('ES',reg) if reg>3000 and reg<4000\r\n",
        "        else ('MSc',reg) if reg>4000 and reg<5000\r\n",
        "        else ('CC',reg) if reg>5000 and reg<6000\r\n",
        "        else ('BDA',reg) if reg>6000 and reg<7000\r\n",
        "        else ('HDA',reg))\r\n",
        "classify.collect()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('BDA', 6027),\n",
              " ('VLSI', 2005),\n",
              " ('VLSI', 2035),\n",
              " ('BDA', 6011),\n",
              " ('HDA', 9007),\n",
              " ('HDA', 9056),\n",
              " ('ES', 3088),\n",
              " ('ES', 3045),\n",
              " ('MSc', 4088),\n",
              " ('MSc', 4065),\n",
              " ('CC', 5077),\n",
              " ('CC', 5066),\n",
              " ('MS', 1001),\n",
              " ('MS', 1002)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9emnLsbBIex"
      },
      "source": [
        "9.Text file contain numbers. Numbers are separated by one white space. \r\n",
        "There is no order to store the numbers. One line may contain one or more numbers.\r\n",
        "Find the maximum, minimum, sum and mean of numbers.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-IQvZ-1Bsgd",
        "outputId": "25616750-b5bc-4bcf-b301-33184ee9d3a1"
      },
      "source": [
        "file1=spark.sparkContext.textFile(\"number.txt\")\r\n",
        "file1_rdd=file1.flatMap(lambda z:z.split(\" \")).map(lambda c:int(c))\r\n",
        "file1_rdd.max()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "217"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeS7X3AxBzhD",
        "outputId": "4cfddaac-b08f-4b1f-a868-8c6c8eb915c4"
      },
      "source": [
        "file1_rdd.min()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8bUn82PB1b6"
      },
      "source": [
        "file1_rdd.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOtAMqhlB32n",
        "outputId": "65c1c3f2-fffd-4d08-e496-9994c445f4f9"
      },
      "source": [
        "file1_rdd.mean()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWiWDrAnB9iI"
      },
      "source": [
        "10. A text file (citizen.txt) contains data about citizens of country. Fields (information in file) are Name, dob, Phone, email and state name. Another file contains mapping of state names to state code like Karnataka is codes as KA, TamilNadu as TN, Kerala KL etc. Compress the citizen.txt file by changing full state name to state code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oywCA-pCCdQ",
        "outputId": "ada5c827-1aec-4cd9-aa57-ccc0eef0bc95"
      },
      "source": [
        "details=spark.sparkContext.textFile(\"citizen.txt\")\r\n",
        "code=spark.sparkContext.textFile(\"state_codes_of_different_states.txt\")\r\n",
        "details_rdd=details.map(lambda x:x.split(\",\")).collect()\r\n",
        "code_rdd=code.map(lambda y:y.split(\",\")).collect()\r\n",
        "for i in range(len(details_rdd)):\r\n",
        "    for j in range(len(code_rdd)):  \r\n",
        "        if details_rdd[i][4]==code_rdd[j][0]:\r\n",
        "            details_rdd[i][4]=code_rdd[j][1]\r\n",
        "details_rdd              "
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['thejas', '12-05-1996', '86573', 'thejas96@gmail.com', 'KA'],\n",
              " ['yashwanth', '01-02-1998', '98786', 'yashwanth@gmail.com', 'JK'],\n",
              " ['Sharanya', '31-01-1996', '98675', 'sharanya@gmail.com', 'AP'],\n",
              " ['Chethana', '12-03-1995', '89523', 'chethana@gmail.com', 'TN'],\n",
              " ['Akshith', '04-07-1993', '78234', 'akshith@gmail.com', 'GJ'],\n",
              " ['Amruta', '21-10-1996', '65432', 'amruta@gmail.com', 'WB'],\n",
              " ['Anupam', '05-09-1993', '87474', 'anupam@gmail.com', 'RJ'],\n",
              " ['Chandan', '13-02-1997', '76893', 'chandan@gmail.com', 'PB'],\n",
              " ['Himana', '04-08-1995', '98234', 'himana@gmail.com', 'JK'],\n",
              " ['Varsha', '05-09-1997', '86574', 'varsha@gmail.com', 'MH'],\n",
              " ['Arun', '17-12-1994', '67895', 'arun@gmail.com', 'UP']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7mCUAnmL1Lr"
      },
      "source": [
        "stRDD = spark.sparkContext.textFile('state_codes_of_different_states.txt')\r\n",
        "stateKey = stRDD.map(lambda word: (word.split(',')[0], word.split(',')[1]))"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8QOH-x-MeAk",
        "outputId": "b12752cb-9325-4842-e2e6-a41b71589a41"
      },
      "source": [
        "#creating dictionary\r\n",
        "code_dict = {}\r\n",
        "for val in stateKey.collect():\r\n",
        "    code_dict[val[0]] = val[1]\r\n",
        "    \r\n",
        "code_dict"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'AndhraPradesh': 'AP',\n",
              " 'Gujarat': 'GJ',\n",
              " 'JammuKashmir': 'JK',\n",
              " 'Karnataka': 'KA',\n",
              " 'Maharashtra': 'MH',\n",
              " 'Punjab': 'PB',\n",
              " 'Rajasthan': 'RJ',\n",
              " 'TamilNadu': 'TN',\n",
              " 'UttarPradesh': 'UP',\n",
              " 'WestBengal': 'WB'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sz5rg7YEMlYd",
        "outputId": "810f19e3-aa46-4eb4-b0d6-1dc3c35509c5"
      },
      "source": [
        "mapData = spark.sparkContext.broadcast(code_dict)\r\n",
        "\r\n",
        "cityRdd = spark.sparkContext.textFile('citizen.txt')\r\n",
        "print(cityRdd.collect())\r\n",
        "def abc(state,codes):\r\n",
        "    splitData = state.split(',')  \r\n",
        "    print(splitData)\r\n",
        "    splitData[4] = codes.value.get(splitData[4])\r\n",
        "    newData = ' '\r\n",
        "    newData = newData.join(splitData)\r\n",
        "    \r\n",
        "    return newData\r\n",
        "    \r\n",
        "mapCitizen = cityRdd.map(lambda data: abc(data,mapData))\r\n",
        "mapCitizen.collect()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['thejas,12-05-1996,86573,thejas96@gmail.com,Karnataka', 'yashwanth,01-02-1998,98786,yashwanth@gmail.com,JammuKashmir', 'Sharanya,31-01-1996,98675,sharanya@gmail.com,AndhraPradesh', 'Chethana,12-03-1995,89523,chethana@gmail.com,TamilNadu', 'Akshith,04-07-1993,78234,akshith@gmail.com,Gujarat', 'Amruta,21-10-1996,65432,amruta@gmail.com,WestBengal', 'Anupam,05-09-1993,87474,anupam@gmail.com,Rajasthan', 'Chandan,13-02-1997,76893,chandan@gmail.com,Punjab', 'Himana,04-08-1995,98234,himana@gmail.com,JammuKashmir', 'Varsha,05-09-1997,86574,varsha@gmail.com,Maharashtra', 'Arun,17-12-1994,67895,arun@gmail.com,UttarPradesh']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['thejas 12-05-1996 86573 thejas96@gmail.com KA',\n",
              " 'yashwanth 01-02-1998 98786 yashwanth@gmail.com JK',\n",
              " 'Sharanya 31-01-1996 98675 sharanya@gmail.com AP',\n",
              " 'Chethana 12-03-1995 89523 chethana@gmail.com TN',\n",
              " 'Akshith 04-07-1993 78234 akshith@gmail.com GJ',\n",
              " 'Amruta 21-10-1996 65432 amruta@gmail.com WB',\n",
              " 'Anupam 05-09-1993 87474 anupam@gmail.com RJ',\n",
              " 'Chandan 13-02-1997 76893 chandan@gmail.com PB',\n",
              " 'Himana 04-08-1995 98234 himana@gmail.com JK',\n",
              " 'Varsha 05-09-1997 86574 varsha@gmail.com MH',\n",
              " 'Arun 17-12-1994 67895 arun@gmail.com UP']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    }
  ]
}